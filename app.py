import io
import os
import re
import time
import shutil
import uuid
import sys
import json
import threading
import zipfile
import random
import string 
import requests
from concurrent.futures import ThreadPoolExecutor
from flask import Flask, request, jsonify, send_file, redirect, after_this_request, Response, stream_with_context
from flask_cors import CORS
from rembg import remove
import yt_dlp

# ==============================================================================
# 0. å…¨å±€é…ç½®ä¸åˆå§‹åŒ–
# ==============================================================================
app = Flask(__name__)
# å…è®¸è·¨åŸŸï¼Œæ”¯æŒå‰ç«¯ main.js çš„è¯·æ±‚
CORS(app, resources={r"/*": {"origins": "*"}})

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOWNLOAD_FOLDER = os.path.join(BASE_DIR, "downloads")
BATCH_TEMP_FOLDER = os.path.join(BASE_DIR, "batch_temp") 
SUBTITLE_TEMP_FOLDER = os.path.join(BASE_DIR, "subtitle_temp")

ARK_API_KEY = "3a4b60e4-f692-4210-b26e-a03c636fc804"
ARK_MODEL = "glm-4-7-251222"
ARK_URL = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
for folder in [DOWNLOAD_FOLDER, BATCH_TEMP_FOLDER, SUBTITLE_TEMP_FOLDER]:
    os.makedirs(folder, exist_ok=True) 

# å…¨å±€ä»»åŠ¡å­˜å‚¨ (åŒæ—¶ç”¨äºæ—§ç‰ˆè§†é¢‘æ‰¹é‡å’Œæ–°ç‰ˆå­—å¹•æ‰¹é‡)
tasks = {}

# ==============================================================================
# 1. åŠ¨æ€ä»£ç†ç”Ÿæˆé€»è¾‘ (å‚è€ƒä½ æä¾›çš„ä»£ç )
# ==============================================================================
def get_proxy_url():
    """ ç”ŸæˆåŠ¨æ€ä½å®…ä»£ç† URLï¼Œç”¨äºç»•è¿‡ YouTube çš„ IP é™åˆ¶ """
    try:
        customer_id = "B_48472"
        password = "vaug9887"
        country = "US"
        gateway = "gate1.ipweb.cc"
        port = "7778"
        # æ¯æ¬¡è¯·æ±‚ç”Ÿæˆä¸åŒçš„ Session IDï¼Œç¡®ä¿ IP åˆ‡æ¢
        session_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
        username = f"{customer_id}_{country}___5_{session_id}"
        return f"http://{username}:{password}@{gateway}:{port}"
    except Exception as e:
        print(f"âš ï¸ ä»£ç†ç”Ÿæˆå¤±è´¥: {e}")
        return None

def sanitize_filename(filename):
    """ æ¸…æ´—æ–‡ä»¶åï¼Œé˜²æ­¢æ–‡ä»¶ç³»ç»Ÿé”™è¯¯ """
    if not filename: return "download"
    # æ›¿æ¢éæ³•å­—ç¬¦
    clean = re.sub(r'[\\/*?:"<>|]', "_", filename).strip()
    # é™åˆ¶é•¿åº¦
    encoded_bytes = clean.encode('utf-8')
    if len(encoded_bytes) > 200:
        clean = encoded_bytes[:200].decode('utf-8', 'ignore')
    return clean

def parse_srt_to_text(content):
    """ è§£æ SRT/VTT ä¸ºçº¯æ–‡æœ¬ (ç”¨äºAIæ€»ç»“) """
    if not content: return ""
    
    lines = content.splitlines()
    cleaned_lines = []
    
    timestamp_pattern = re.compile(r'\d{2}:\d{2}:\d{2}[,.]\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}[,.]\d{3}')
    
    for line in lines:
        line = line.strip()
        if not line: continue
        if line.upper().startswith('WEBVTT') or line.startswith('Language:') or line.startswith('Kind:'): 
            continue
        if line.isdigit(): continue
        if timestamp_pattern.search(line): continue
        text_line = re.sub(r'<[^>]+>', '', line)
        text_line = text_line.strip()
        
        if text_line:
            cleaned_lines.append(text_line)
    
    # ç®€å•å»é‡
    unique_lines = []
    for i, line in enumerate(cleaned_lines):
        if i > 0 and line == cleaned_lines[i-1]:
            continue
        unique_lines.append(line)
        
    return '\n'.join(unique_lines)

def get_video_info_core(video_url):
    """ è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯ (ç”¨äºå‰ç«¯ /api/info) """
    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy, 'quiet': True, 'no_warnings': True, 'skip_download': True,
        'extractor_args': {'youtube': {'player_client': ['ios', 'android', 'web']}}
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl: 
            info = ydl.extract_info(video_url, download=False)
            
            # å¤„ç†å­—å¹•åˆ—è¡¨
            ALLOWED_LANGS = ['en', 'zh', 'ja', 'ko', 'es', 'fr', 'de', 'ru']
            subtitles_list = []
            # æå–æ‰‹åŠ¨å­—å¹•
            for lang, subs in info.get('subtitles', {}).items():
                if any(lang.startswith(al) for al in ALLOWED_LANGS):
                    subtitles_list.append({"lang_code": lang, "name": subs[0].get('name', lang), "is_auto": False})
            # æå–è‡ªåŠ¨å­—å¹•
            for lang, subs in info.get('automatic_captions', {}).items():
                if any(lang.startswith(al) for al in ALLOWED_LANGS):
                    if not any(s['lang_code'] == lang for s in subtitles_list):
                        subtitles_list.append({"lang_code": lang, "name": subs[0].get('name', lang), "is_auto": True})
            
            subtitles_list.sort(key=lambda x: 0 if x['lang_code'].startswith('en') else 1)

            return {
                "url": video_url, 
                "title": info.get('title'), 
                "thumbnail": info.get('thumbnail'),
                "duration": info.get('duration'),
                "uploader": info.get('uploader'),
                "formats": process_video_formats(info), 
                "subtitles": subtitles_list
            }, None
    except Exception as e:
        return {"url": video_url, "error": str(e)}, str(e)

def process_video_formats(info):
    """ æ ¼å¼åŒ–è§†é¢‘æµä¿¡æ¯ """
    formats_raw = info.get('formats', [])
    final_formats = []
    for f in formats_raw:
        if f.get('vcodec') != 'none' and f.get('height'):
            final_formats.append({
                "format_id": f.get('format_id'),
                "resolution": f"{f.get('height')}p",
                "filesize": f.get('filesize'),
                "type": "video"
            })
        elif f.get('vcodec') == 'none' and f.get('acodec') != 'none':
            final_formats.append({
                "format_id": f.get('format_id'),
                "resolution": "Audio",
                "filesize": f.get('filesize'),
                "type": "audio"
            })
    return final_formats

def run_download_video_core(video_url, quality_val, output_dir):
    """ æœåŠ¡å™¨ä¸‹è½½è§†é¢‘/éŸ³é¢‘æ ¸å¿ƒé€»è¾‘ """
    unique_id = str(uuid.uuid4())
    output_template = os.path.join(output_dir, f"{unique_id}.%(ext)s")
    
    if quality_val == 'audio':
        dl_format, final_ext = 'bestaudio/best', 'mp3'
    elif quality_val and quality_val.isdigit():
        dl_format = f'bestvideo[height<={quality_val}]+bestaudio/best[height<={quality_val}]/best'
        final_ext = 'mp4'
    else:
        dl_format, final_ext = 'bestvideo+bestaudio/best', 'mp4'
    
    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy, 'format': dl_format, 'outtmpl': output_template,
        'quiet': False, 'no_warnings': True,
        'merge_output_format': 'mp4' if final_ext == 'mp4' else None
    }
    if final_ext == 'mp3':
        ydl_opts['postprocessors'] = [{'key': 'FFmpegExtractAudio','preferredcodec': 'mp3','preferredquality': '192'}]

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            candidates = [f for f in os.listdir(output_dir) if f.startswith(unique_id)]
            if not candidates: return None, None, "File not generated"
            
            final_filepath = os.path.join(output_dir, candidates[0])
            real_ext = os.path.splitext(final_filepath)[1]
            friendly_filename = f"{sanitize_filename(info.get('title', 'video'))}{real_ext}"
            return final_filepath, friendly_filename, None
    except Exception as e:
        return None, None, str(e)

def check_video_for_batch(video_url):
    """ å¯¹åº” /api/batch_check """
    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy, 
        'quiet': True, 
        'no_warnings': True, 
        'skip_download': True,
        'extract_flat': True, 
        'extractor_args': {'youtube': {'player_client': ['android', 'web']}}
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl_opts['extract_flat'] = False 
            info = ydl.extract_info(video_url, download=False)
            has_subtitles = bool(info.get('subtitles') or info.get('automatic_captions'))
            return {
                "url": video_url,
                "video_info": {
                    "title": info.get('title', 'Unknown'), 
                    "uploader": info.get('uploader', 'Unknown')
                },
                "can_download": has_subtitles
            }
    except Exception as e:
        return {
            "url": video_url, 
            "video_info": {"title": "Error/Invalid", "uploader": "Unknown"}, 
            "can_download": False
        }

# å­—å¹•å†…å­˜ç¼“å­˜ {cache_key: (timestamp, transcript, title)}
_subtitle_cache: dict = {}
_SUBTITLE_CACHE_TTL = 3600  # 1å°æ—¶

def get_subtitle_fast(video_url: str, lang_code: str = 'en'):
    """
    å¿«é€Ÿå­—å¹•è·å–ï¼šyt_dlp åªæ‹¿å…ƒæ•°æ®æ‹¿åˆ°å­—å¹• URLï¼Œå†ç›´æ¥ requests.get ä¸‹è½½å†…å®¹ã€‚
    è·³è¿‡ yt_dlp æ–‡ä»¶å†™å…¥ + FFmpeg è½¬æ¢ï¼Œé€Ÿåº¦æå‡ 3-5xã€‚
    å¸¦å†…å­˜ç¼“å­˜ï¼ŒåŒä¸€è§†é¢‘ 1 å°æ—¶å†…ä¸é‡å¤æŠ“å–ã€‚
    """
    cache_key = f"{video_url}_{lang_code}"
    now = time.time()

    # å‘½ä¸­ç¼“å­˜ç›´æ¥è¿”å›
    if cache_key in _subtitle_cache:
        ts, transcript, title = _subtitle_cache[cache_key]
        if now - ts < _SUBTITLE_CACHE_TTL:
            return transcript, title, None

    proxy = get_proxy_url()
    ydl_opts = {'proxy': proxy, 'skip_download': True, 'quiet': True, 'no_warnings': True}

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)

        title = info.get('title', 'Video')

        # ä¼˜å…ˆæ‰‹åŠ¨å­—å¹•ï¼Œå…¶æ¬¡è‡ªåŠ¨å­—å¹•
        sub_url = None
        for source in [info.get('subtitles', {}), info.get('automatic_captions', {})]:
            for lang in [lang_code, 'en', 'en-US', 'en-orig']:
                if lang in source:
                    for fmt in source[lang]:
                        if fmt.get('ext') in ('vtt', 'json3', 'srv3'):
                            sub_url = fmt['url']
                            break
                    if sub_url:
                        break
            if sub_url:
                break

        if not sub_url:
            return None, None, "No subtitles found for this language."

        # ç›´æ¥ HTTP ä¸‹è½½å­—å¹•ï¼Œä¸èµ° yt_dlp ä¸‹è½½æœºåˆ¶ï¼ˆæ— ç£ç›˜ IOï¼Œæ—  FFmpegï¼‰
        proxies = {'http': proxy, 'https': proxy} if proxy else None
        resp = requests.get(sub_url, timeout=30, proxies=proxies)
        resp.raise_for_status()
        transcript = parse_srt_to_text(resp.text)

        # å†™å…¥ç¼“å­˜
        _subtitle_cache[cache_key] = (now, transcript, title)
        return transcript, title, None

    except Exception as e:
        return None, None, str(e)


def get_subtitle_content(video_url, lang_code, output_format='srt'):
    proxy = get_proxy_url()
    unique_id = str(uuid.uuid4())
    output_path_base = os.path.join(SUBTITLE_TEMP_FOLDER, unique_id)
    
    target_dl_fmt = 'srt' if output_format == 'srt' else 'vtt'

    ydl_opts = {
        'proxy': proxy,
        'skip_download': True,
        'writesubtitles': True,
        'writeautomaticsub': True, 
        'subtitleslangs': [lang_code], 
        'outtmpl': output_path_base,
        'quiet': True,
        'no_warnings': True,
        'postprocessors': [{'key': 'FFmpegSubtitlesConvertor', 'format': target_dl_fmt}],
    }

    generated_file = None
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            title = info.get('title', 'subtitle')

        for f in os.listdir(SUBTITLE_TEMP_FOLDER):
            if f.startswith(unique_id):
                generated_file = os.path.join(SUBTITLE_TEMP_FOLDER, f)
                break
        
        if not generated_file:
            return None, None, "No subtitles found for this language."

        with open(generated_file, 'r', encoding='utf-8') as f: 
            content = f.read()

        final_ext = output_format
        if output_format == 'txt':
            content = parse_srt_to_text(content)
        
        safe_filename = f"{sanitize_filename(title)}.{final_ext}"
        return content, safe_filename, None

    except Exception as e:
        return None, None, str(e)
    finally:
        if generated_file and os.path.exists(generated_file):
            try: os.remove(generated_file)
            except: pass

# ==============================================================================
# 2. æ ¸å¿ƒè§£ææ¥å£
# ==============================================================================
@app.route('/api/get-parse', methods=['POST', 'OPTIONS'])
def get_parse_video():
    # å¤„ç† CORS é¢„æ£€è¯·æ±‚
    if request.method == 'OPTIONS':
        return '', 204

    data = request.get_json()
    video_url = data.get('url')
    
    if not video_url:
        return jsonify({"error": "No URL provided"}), 400

    print(f"ğŸš€ æ”¶åˆ°è§£æè¯·æ±‚: {video_url}")

    # è·å–åŠ¨æ€ä»£ç†
    proxy = get_proxy_url()
    
    # yt-dlp é…ç½®ä¼˜åŒ–
    ydl_opts = {
        'proxy': proxy,
        # å¼ºåˆ¶é€‰æ‹©åŒ…å«éŸ³è§†é¢‘çš„å•æ–‡ä»¶æ ¼å¼ (itag 22=720p, 18=360p)
        'format': '22/18/best[ext=mp4][vcodec^=avc1]/best',
        'quiet': True,
        'no_warnings': True,
        # å¿…é¡»ä¸ Cloudflare Worker ä¸­çš„ User-Agent ä¿æŒé«˜åº¦ä¸€è‡´ï¼Œé˜²æ­¢ 403
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'nocheckcertificate': True,
        # å…³é”®ï¼šç”±äºä½¿ç”¨ä»£ç†ï¼Œæˆ‘ä»¬éœ€è¦è¿™äº›å‚æ•°æ¨¡æ‹ŸçœŸå®ç§»åŠ¨/ç½‘é¡µå®¢æˆ·ç«¯
        'extractor_args': {
            'youtube': {
                'player_client': ['android', 'web'],
                'skip': ['hls', 'dash'] # è·³è¿‡åˆ‡ç‰‡æµï¼Œå¼ºåˆ¶æ‰¾å•æ–‡ä»¶
            }
        }
    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # æå–ä¿¡æ¯ä¸ä¸‹è½½
            info = ydl.extract_info(video_url, download=False)
            languages = [] 
            # è·å–äººå·¥ç”Ÿæˆå­—å¹•
            subs = info.get('subtitles', {})
            for code, items in subs.items():
                languages.append({
                    "code": code,
                    "label": f"{items[0].get('name', code)} (Original)",
                    "is_auto": False
                })
           
           
            # è·å–è‡ªåŠ¨ç”Ÿæˆå­—å¹•
            auto_subs = info.get('automatic_captions', {})
            for code, items in auto_subs.items():
                # è¿‡æ»¤æ‰å·²ç»å­˜åœ¨çš„äººå·¥å­—å¹•ä»£ç 
                if not any(l['code'] == code for l in languages):
                    languages.append({
                        "code": code,
                        "label": f"{items[0].get('name', code)} (Auto-generated)",
                        "is_auto": True
                    })

            # è¿”å›ç»™å‰ç«¯çš„ç»“æœ
            result = {
                "title": info.get('title'),
                "url": info.get('url'),  # è¿™é‡Œçš„ URL å°†äº¤ç»™ Worker è½¬å‘
                "thumbnail": info.get('thumbnail'),
                "ext": info.get('ext') or 'mp4',
                "languages": languages,
                "size": info.get('filesize') or info.get('filesize_approx'),
                "proxy_used": True if proxy else False
            }
            
            print(f"âœ… è§£ææˆåŠŸ: {info.get('title')[:30]}...")
            return jsonify(result)

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ è§£æå‡ºé”™: {error_msg}")
        
        # é’ˆå¯¹å¸¸è§é”™è¯¯çš„å‹å¥½æç¤º
        if "bot" in error_msg.lower():
            return jsonify({"error": "Please try again later."}), 500
        return jsonify({"error": error_msg}), 500



# ==============================================================================
# 3. æ‰¹é‡è§£ææ¥å£ (æ”¯æŒæœ€å¤š 3 ä¸ªè§†é¢‘)
# ==============================================================================
@app.route('/api/get-parse-batch', methods=['POST', 'OPTIONS'])
def get_parse_batch():
    if request.method == 'OPTIONS':
        return '', 204

    data = request.get_json()
    urls = data.get('urls', [])
    
    if not urls or not isinstance(urls, list):
        return jsonify({"error": "è¯·æä¾›æœ‰æ•ˆçš„ URL åˆ—è¡¨"}), 400

    # é™åˆ¶æœ€å¤§æ‰¹é‡æ•°ä¸º 3ï¼Œé˜²æ­¢è¢« YouTube å°é” IP
    urls = urls[:3]
    print(f"ğŸš€ æ”¶åˆ°æ‰¹é‡è§£æè¯·æ±‚: {len(urls)} ä¸ªè§†é¢‘")

    proxy = get_proxy_url()
    
    # ç»Ÿä¸€å®šä¹‰è§£æå•ä¸ªè§†é¢‘çš„å‡½æ•°
    def parse_single(url):
        ydl_opts = {
            'proxy': proxy,
            'format': '22/18/best[ext=mp4][vcodec^=avc1]/best',
            'quiet': True,
            'no_warnings': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'nocheckcertificate': True,
            'extractor_args': {
                'youtube': {
                    'player_client': ['android', 'web'],
                    'skip': ['hls', 'dash']
                }
            }
        }
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                return {
                    "title": info.get('title'),
                    "url": info.get('url'), # è§†é¢‘ç›´é“¾
                    "thumbnail": info.get('thumbnail'),
                    "status": "success"
                }
        except Exception as e:
            return {"url": url, "status": "failed", "error": str(e)}

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è§£æï¼Œæé«˜é€Ÿåº¦
    results = []
    with ThreadPoolExecutor(max_workers=3) as executor:
        results = list(executor.map(parse_single, urls))

    return jsonify({"results": results})


# ==============================================================================
# 4. åå°ä»»åŠ¡çº¿ç¨‹
# ==============================================================================

# æ—§ç‰ˆè§†é¢‘æ‰¹é‡ä¸‹è½½çº¿ç¨‹ (ä¿æŒåŸæ ·)
def batch_video_worker(task_id, video_list):
    task_dir = os.path.join(BATCH_TEMP_FOLDER, task_id)
    os.makedirs(task_dir, exist_ok=True)
    total = len(video_list)
    success_files = []
    
    for index, item in enumerate(video_list):
        url = item.get('url')
        quality = item.get('quality', 'best')
        
        tasks[task_id]['progress'] = f"{index}/{total}"
        tasks[task_id]['status'] = "processing"
        
        filepath, filename, err = run_download_video_core(url, quality, task_dir)
        if filepath and not err: success_files.append(filepath)
        time.sleep(1)
            
    if success_files:
        tasks[task_id]['status'] = "completed"
        tasks[task_id]['progress'] = f"{total}/{total}"
        zip_name = f"Video_Batch_{task_id[:8]}.zip"
        zip_path = os.path.join(DOWNLOAD_FOLDER, zip_name)
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for file in success_files: zf.write(file, arcname=os.path.basename(file))
        tasks[task_id]['zip_path'] = zip_path
        tasks[task_id]['zip_name'] = zip_name
    else:
        tasks[task_id]['status'] = "failed"
    shutil.rmtree(task_dir, ignore_errors=True)

# æ–°ç‰ˆå­—å¹•æ‰¹é‡ä¸‹è½½çº¿ç¨‹ (é€‚é… main.js)
def batch_subtitle_worker(task_id, videos, lang, fmt):
    task_dir = os.path.join(BATCH_TEMP_FOLDER, task_id)
    os.makedirs(task_dir, exist_ok=True)
    total = len(videos)
    success_files = []

    for i, video in enumerate(videos):
        url = video.get('url')
        tasks[task_id]['progress'] = f"{i}/{total}" 
        
        content, filename, err = get_subtitle_content(url, lang, fmt)
        
        if content and not err:
            file_path = os.path.join(task_dir, filename)
            counter = 1
            while os.path.exists(file_path):
                name, ext = os.path.splitext(filename)
                file_path = os.path.join(task_dir, f"{name}_{counter}{ext}")
                counter += 1
            
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                success_files.append(file_path)
            except: pass
        
        time.sleep(random.uniform(0.5, 1.5))

    tasks[task_id]['progress'] = f"{total}/{total}"
    
    if success_files:
        zip_name = f"Subtitles_Batch_{task_id[:8]}.zip"
        zip_path = os.path.join(DOWNLOAD_FOLDER, zip_name)
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for fp in success_files: zf.write(fp, arcname=os.path.basename(fp))
        
        tasks[task_id]['zip_path'] = zip_path
        tasks[task_id]['zip_name'] = zip_name
        tasks[task_id]['status'] = "completed"
    else:
        tasks[task_id]['status'] = "failed"
    
    shutil.rmtree(task_dir, ignore_errors=True)


# ==============================================================================
# 4. Playlist/Channel è§£ææ¥å£ (æ–°å¢)
# ==============================================================================

def extract_playlist_videos(playlist_url, max_videos=50):
    """ä»playlist URLæå–æ‰€æœ‰è§†é¢‘URL - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¤§åˆ—è¡¨åˆ†çº§å¤„ç†"""
    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy,
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,  # è½»é‡åŒ–å±•å¼€ï¼Œåªè·å–åŸºæœ¬ä¿¡æ¯
        'extractor_args': {'youtube': {'player_client': ['android', 'web']}},
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'nocheckcertificate': True,
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(playlist_url, download=False)
            
            if not info:
                return [], "Failed to extract playlist information"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰entries
            if 'entries' not in info or not info['entries']:
                return [], "No videos found in playlist or channel"
            
            videos = []
            processed_count = 0
            
            # è·å–playlist/channelçš„åŸºæœ¬ä¿¡æ¯
            playlist_title = info.get('title', 'Unknown Playlist')
            playlist_uploader = info.get('uploader', 'Unknown')
            total_entries = len([e for e in info['entries'] if e])  # è¿‡æ»¤Noneæ¡ç›®
            
            print(f"ğŸ“‹ Processing playlist: {playlist_title} ({total_entries} total videos)")
            
            for entry in info['entries']:
                if processed_count >= max_videos:
                    print(f"âš ï¸ Reached max limit ({max_videos}), stopping extraction")
                    break
                    
                # è·³è¿‡æ— æ•ˆæ¡ç›®
                if not entry or not entry.get('id'):
                    continue
                
                try:
                    video_url = f"https://www.youtube.com/watch?v={entry['id']}"
                    video_data = {
                        'url': video_url,
                        'title': sanitize_filename(entry.get('title', 'Unknown')),
                        'id': entry['id'],
                        'duration': entry.get('duration', 0),
                        'uploader': entry.get('uploader', playlist_uploader),
                        'playlist_title': playlist_title,  # æ–°å¢ï¼šè®°å½•æ¥æºplaylist
                        'playlist_index': entry.get('playlist_index', processed_count + 1)  # æ–°å¢ï¼šè®°å½•åœ¨playlistä¸­çš„ä½ç½®
                    }
                    videos.append(video_data)
                    processed_count += 1
                except Exception as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆè§†é¢‘æ¡ç›®: {e}")
                    continue
            
            print(f"âœ… Successfully extracted {len(videos)} videos from playlist")
            return videos, None
            
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Playlistè§£æå‡ºé”™: {error_msg}")
        return [], error_msg

def deduplicate_videos(all_videos):
    """å»é‡é€»è¾‘ï¼šæ ¹æ®video_idå»é™¤é‡å¤è§†é¢‘"""
    seen_ids = set()
    unique_videos = []
    duplicates_count = 0
    
    for video in all_videos:
        video_id = video.get('id')
        if video_id and video_id not in seen_ids:
            seen_ids.add(video_id)
            unique_videos.append(video)
        else:
            duplicates_count += 1
    
    if duplicates_count > 0:
        print(f"ğŸ”„ Removed {duplicates_count} duplicate videos")
    
    return unique_videos

@app.route('/api/parse-playlist', methods=['POST', 'OPTIONS'])
def parse_playlist():
    """è§£æplaylistæˆ–channelï¼Œè¿”å›è§†é¢‘åˆ—è¡¨"""
    if request.method == 'OPTIONS':
        return '', 204
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400
        
    url = data.get('url')
    max_videos = data.get('max_videos', 50)  # é»˜è®¤æœ€å¤š50ä¸ª
    
    if not url:
        return jsonify({"error": "No URL provided"}), 400
    
    # é™åˆ¶æœ€å¤§è§†é¢‘æ•°é‡ï¼Œé˜²æ­¢æ»¥ç”¨
    max_videos = min(max_videos, 100)  # ç¡¬é™åˆ¶100ä¸ª
    
    print(f"ğŸµ æ”¶åˆ°playlistè§£æè¯·æ±‚: {url[:100]}...")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯playlistæˆ–channel URL
    playlist_patterns = [
        'playlist?list=',
        '/channel/',
        '/@',
        '/c/',
        '/user/'
    ]
    
    if not any(pattern in url for pattern in playlist_patterns):
        return jsonify({"error": "Not a valid playlist or channel URL"}), 400
    
    videos, error = extract_playlist_videos(url, max_videos)
    
    if error:
        return jsonify({"error": error}), 500
    
    if not videos:
        return jsonify({"error": "No videos found in playlist/channel"}), 404
    
    # ç¡®å®šç±»å‹
    url_type = "playlist" if "playlist?list=" in url else "channel"
    
    return jsonify({
        "type": url_type,
        "total_videos": len(videos),
        "videos": videos,
        "source_url": url,
        "max_requested": max_videos
    })

@app.route('/api/parse-mixed', methods=['POST', 'OPTIONS'])
def parse_mixed_urls():
    """æ™ºèƒ½è§£ææ··åˆURLï¼šè‡ªåŠ¨è¯†åˆ«å•ä¸ªè§†é¢‘ã€playlistã€channelï¼Œæ”¯æŒå»é‡å’Œåˆ†çº§å¤„ç†"""
    # å¤„ç† CORS é¢„æ£€è¯·æ±‚
    if request.method == 'OPTIONS':
        return '', 204
    
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400
        
    urls = data.get('urls', [])
    
    if not urls or not isinstance(urls, list):
        return jsonify({"error": "No URLs provided or invalid format"}), 400
    
    # é™åˆ¶URLæ•°é‡ï¼Œé˜²æ­¢æ»¥ç”¨
    urls = urls[:10]  # æœ€å¤š10ä¸ªURL
    
    print(f"ğŸš€ æ”¶åˆ°æ··åˆURLè§£æè¯·æ±‚: {len(urls)} ä¸ªURL")
    
    results = []
    all_videos = []  # ç”¨äºæœ€ç»ˆå»é‡
    playlist_info = []
    
    for url in urls:
        if not url or not isinstance(url, str):
            results.append({"url": url, "type": "error", "error": "Invalid URL format"})
            continue
            
        try:
            # æ£€æŸ¥URLç±»å‹
            playlist_patterns = [
                'playlist?list=',
                '/channel/',
                '/@',
                '/c/',
                '/user/'
            ]
            
            if any(pattern in url for pattern in playlist_patterns):
                # Playlist/Channelå¤„ç† - ä½¿ç”¨åˆ†çº§å¤„ç†
                max_per_playlist = 100  # æ¯ä¸ªplaylistæœ€å¤š100ä¸ªè§†é¢‘
                videos, error = extract_playlist_videos(url, max_per_playlist)
                
                if error:
                    results.append({"url": url, "type": "error", "error": error})
                else:
                    # æ”¶é›†playlistä¿¡æ¯
                    playlist_info.append({
                        "url": url,
                        "title": videos[0].get('playlist_title', 'Unknown Playlist') if videos else 'Empty Playlist',
                        "count": len(videos)
                    })
                    
                    # æ·»åŠ åˆ°æ€»è§†é¢‘åˆ—è¡¨
                    all_videos.extend(videos)
                    
                    results.append({
                        "url": url, 
                        "type": "playlist", 
                        "videos": videos,
                        "count": len(videos),
                        "playlist_title": videos[0].get('playlist_title', 'Unknown') if videos else 'Unknown'
                    })
            else:
                # å•ä¸ªè§†é¢‘å¤„ç† - å¤ç”¨ç°æœ‰å‡½æ•°
                video_info, error = get_video_info_core(url)
                if error:
                    results.append({"url": url, "type": "error", "error": error})
                else:
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼å¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
                    video_id = None
                    if 'watch?v=' in url:
                        import re
                        match = re.search(r'[?&]v=([^&#]+)', url)
                        video_id = match.group(1) if match else url[-11:]
                    else:
                        video_id = url[-11:]
                    
                    video_data = {
                        'url': url,
                        'title': video_info.get('title', 'Unknown'),
                        'id': video_id,
                        'duration': video_info.get('duration', 0),
                        'uploader': video_info.get('uploader', 'Unknown')
                    }
                    all_videos.append(video_data)
                    
                    results.append({
                        "url": url,
                        "type": "video",
                        "video_info": video_info
                    })
                    
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ å¤„ç†URLå‡ºé”™ {url}: {error_msg}")
            results.append({"url": url, "type": "error", "error": error_msg})
    
    # æ‰§è¡Œå»é‡é€»è¾‘
    unique_videos = deduplicate_videos(all_videos)
    total_unique = len(unique_videos)
    total_original = len(all_videos)
    duplicates_removed = total_original - total_unique
    
    return jsonify({
        "results": results,
        "summary": {
            "total_videos": total_unique,
            "original_count": total_original,
            "duplicates_removed": duplicates_removed,
            "playlists_processed": len(playlist_info),
            "playlist_info": playlist_info
        },
        "unique_videos": unique_videos  # å»é‡åçš„å®Œæ•´è§†é¢‘åˆ—è¡¨
    })

# ==============================================================================
# 5. éŸ³é¢‘ç›´é“¾è§£ææ¥å£ (ä¼ªè£…æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨ format 18)
# ==============================================================================
@app.route('/api/get-audio-parse', methods=['POST', 'OPTIONS'])
def get_audio_parse():
    if request.method == 'OPTIONS':
        return '', 204

    data = request.get_json()
    video_url = data.get('url')
    
    if not video_url:
        return jsonify({"error": "No URL provided"}), 400

    print(f"ğŸµ æ”¶åˆ°éŸ³é¢‘è§£æè¯·æ±‚ (å¼ºåˆ¶ F18 æ¨¡å¼): {video_url}")
    proxy = get_proxy_url()
    
    ydl_opts = {
        'proxy': proxy,
        'quiet': True,
        'no_warnings': True, 
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'nocheckcertificate': True,
        'extractor_args': {
            'youtube': {
                'player_client': ['android', 'web'],
            }
        },
        'skip_download': True,

    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
            
            # --- æ ¸å¿ƒè¿‡æ»¤é€»è¾‘å¼€å§‹ ---
            all_formats = info.get('formats', [])
            
            # ä»…ä¿ç•™ format_id ä¸º "18" çš„é¡¹
            f18_list = [f for f in all_formats if f.get('format_id') == '18']
            
            # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„ç›´é“¾åœ°å€
            # å¦‚æœæ‰¾åˆ°äº† format 18ï¼Œå°±ç”¨ 18ï¼›å¦åˆ™ç”¨ info é»˜è®¤è¿”å›çš„
            final_url = info.get('url')
            final_filesize = info.get('filesize') or info.get('filesize_approx')

            if f18_list:
                f18 = f18_list[0]
                final_url = f18.get('url')
                final_filesize = f18.get('filesize') or f18.get('filesize_approx')
            # --- æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ç»“æŸ ---

            # æ¸…ç†æ ‡é¢˜
            raw_title = info.get('title', 'audio')
            safe_title = re.sub(r'[\\/*?:"<>|]', "_", raw_title).rstrip()

            result = {
                "title": safe_title,
                "url": final_url,           # ä¼˜å…ˆçº§æœ€é«˜çš„ format 18 ç›´é“¾
                "thumbnail": info.get('thumbnail'),
                "duration": info.get('duration'),
                "ext": "m4a",               # å¼ºåˆ¶åç¼€ä¸º m4a å®ç°ä¼ªè£…
                "filesize": final_filesize, # å¯¹åº” format 18 çš„å¤§å°
                "ua": ydl_opts['user_agent'],
                "formats": f18_list         # åªåŒ…å« format 18 çš„æ•°ç»„
            }
            
            print(f"âœ… è§£ææˆåŠŸ (å·²è¿‡æ»¤ä¸º F18): {safe_title[:30]}...")
            return jsonify(result)

    except Exception as e:
        print(f"âŒ è§£æå‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500



















def format_time(seconds):
    """ å°†ç§’æ•°è½¬æ¢ä¸º MM:SS æ ¼å¼ """
    m, s = divmod(int(seconds), 60)
    return f"{m:02d}:{s:02d}"

def clean_subtitle_text(text):
    """ æ¸…æ´—å­—å¹•æ–‡æœ¬ï¼šå»é™¤ HTML æ ‡ç­¾ã€å¤šä½™ç©ºæ ¼å’Œ YouTube VTT çš„é‡å¤æ ‡è®° """
    # å»é™¤ <c>...</c> æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)

    text = text.replace('&nbsp;', ' ').replace('&gt;', '>').replace('&lt;', '<')
    # å»é™¤é‡å¤çš„æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
    text = text.replace('\n', ' ').strip()
    return text

def parse_vtt_to_segments(vtt_content):
    """ 
    è§£æ VTT æ ¼å¼å¹¶æ·±åº¦å»é‡ (è§£å†³ YouTube è‡ªåŠ¨å­—å¹•æ»šåŠ¨é‡å¤é—®é¢˜)
    """
    raw_segments = []
    # åŒ¹é…æ—¶é—´è½´
    pattern = re.compile(r'(\d{2}:\d{2}:\d{2}.\d{3}) --> (\d{2}:\d{2}:\d{2}.\d{3})')
    
    lines = vtt_content.splitlines()
    
    # ç¬¬ä¸€æ­¥ï¼šåˆæ­¥æå–æ‰€æœ‰æœ‰æ•ˆçš„æ—¶é—´è½´å’Œæ–‡æœ¬
    for i in range(len(lines)):
        match = pattern.search(lines[i])
        if match:
            start_time_str = match.group(1)
            h, m, s = start_time_str.split(':')
            total_seconds = int(h)*3600 + int(m)*60 + float(s)
            
            # æå–ç´§éšæ—¶é—´è½´åçš„æ–‡æœ¬è¡Œ
            current_text = ""
            j = i + 1
            while j < len(lines) and not pattern.search(lines[j]) and lines[j].strip() != "":
                current_text += lines[j] + " "
                j += 1
            
            clean_txt = clean_subtitle_text(current_text)
            if clean_txt:
                raw_segments.append({
                    "t_val": total_seconds, # ç”¨äºæ’åºå’Œæ¯”è¾ƒ
                    "t_str": format_time(total_seconds),
                    "txt": clean_txt
                })

    # ç¬¬äºŒæ­¥ï¼šæ·±åº¦å»é‡æ ¸å¿ƒé€»è¾‘
    # é’ˆå¯¹ YouTube æ»šåŠ¨å­—å¹•ï¼šå¦‚æœä¸‹ä¸€è¡ŒåŒ…å«å½“å‰è¡Œï¼Œåˆ™ä¸¢å¼ƒå½“å‰è¡Œ
    filtered_segments = []
    for i in range(len(raw_segments)):
        current = raw_segments[i]
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€è¡Œï¼Œæ£€æŸ¥ä¸‹ä¸€è¡Œ
        if i < len(raw_segments) - 1:
            next_seg = raw_segments[i + 1]
            # 1. å¦‚æœä¸‹ä¸€è¡Œæ˜¯ä»¥å½“å‰è¡Œå¼€å¤´çš„ (æ»šåŠ¨ç´¯åŠ )
            # 2. æˆ–è€…ä¸‹ä¸€è¡Œå®Œå…¨ç­‰äºå½“å‰è¡Œ (å†—ä½™)
            if next_seg['txt'].startswith(current['txt']) or next_seg['txt'] == current['txt']:
                continue 
        
        # è¡¥å……ï¼šå¦‚æœå½“å‰è¡ŒåŒ…å«ä¸Šä¸€è¡Œ (é’ˆå¯¹æŸäº› VTT ä¹±åºæƒ…å†µ)
        if filtered_segments and current['txt'].startswith(filtered_segments[-1]['txt']):
            filtered_segments[-1] = {
                "t": current['t_str'],
                "txt": current['txt']
            }
        else:
            filtered_segments.append({
                "t": current['t_str'],
                "txt": current['txt']
            })
                
    return filtered_segments
# ==============================================================================
# 6. å­—å¹•æ ¸å¿ƒæ¥å£ (æ–°å¢)
# ==============================================================================

@app.route('/api/transcript/info', methods=['POST'])
def get_transcript_info():
    """ 1. è·å–å­—å¹•å…ƒæ•°æ®ï¼šè¯­è¨€åˆ—è¡¨ """
    data = request.get_json()
    video_url = data.get('url')
    if not video_url:
        return jsonify({"error": "No URL provided"}), 400

    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy,
        'skip_download': True,
        'quiet': True
    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
            
            languages = []
            # è·å–äººå·¥ä¸Šä¼ å­—å¹•
            subs = info.get('subtitles', {})
            for code, items in subs.items():
                languages.append({
                    "code": code,
                    "label": f"{items[0].get('name', code)} (Original)",
                    "is_auto": False
                })
            
            # è·å–è‡ªåŠ¨ç”Ÿæˆå­—å¹•
            auto_subs = info.get('automatic_captions', {})
            for code, items in auto_subs.items():
                # è¿‡æ»¤æ‰å·²ç»å­˜åœ¨çš„äººå·¥å­—å¹•ä»£ç 
                if not any(l['code'] == code for l in languages):
                    languages.append({
                        "code": code,
                        "label": f"{items[0].get('name', code)} (Auto-generated)",
                        "is_auto": True
                    })

            return jsonify({
                "id": info.get('id'),
                "title": info.get('title'),
                "thumbnail": info.get('thumbnail'),
                "languages": languages,
                "default_lang": "en" if any(l['code'] == 'en' for l in languages) else (languages[0]['code'] if languages else "")
            })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/transcript/content', methods=['POST'])
def get_transcript_content():
    """ 2. è·å–æ¸…æ´—åçš„å­—å¹•å†…å®¹ """
    data = request.get_json()
    video_url = data.get('url')
    lang = data.get('lang', 'en')
    
    if not video_url:
        return jsonify({"error": "No URL provided"}), 400

    proxy = get_proxy_url()
    # å”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºä¸´æ—¶æ–‡ä»¶å
    uid = str(uuid.uuid4())
    temp_out = f"temp_sub_{uid}"
    
    ydl_opts = {
        'proxy': proxy,
        'skip_download': True,
        'writesubtitles': True,
        'writeautomaticsub': True,
        'subtitleslangs': [lang],
        'outtmpl': temp_out,
        'quiet': True,
        'postprocessors': [{ 'key': 'FFmpegSubtitlesConvertor', 'format': 'vtt' }]
    }

    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([video_url])
            
        # å¯»æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
        expected_file = f"{temp_out}.{lang}.vtt"
        if not os.path.exists(expected_file):
            # å…¼å®¹æ€§å¤„ç†ï¼šæœ‰äº›è§†é¢‘å¯èƒ½ä¸å¸¦è¯­è¨€åç¼€
            expected_file = f"{temp_out}.vtt"

        if os.path.exists(expected_file):
            with open(expected_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            segments = parse_vtt_to_segments(content)
            full_text = " ".join([s['txt'] for s in segments])
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(expected_file)
            
            return jsonify({
                "format": "json",
                "segments": segments,
                "full_text": full_text
            })
        else:
            return jsonify({"error": "Subtitle file not generated"}), 404
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500




@app.route('/api/transcript/download', methods=['GET'])
def download_transcript():
    """ 3. å­—å¹•æ–‡ä»¶ä¸‹è½½æ¥å£ (ä¿®å¤ç‰ˆ) """
    video_url = request.args.get('url')
    lang = request.args.get('lang', 'en')
    dl_type = request.args.get('type', 'srt') # srt æˆ– txt
    
    if not video_url:
        return "Missing URL", 400

    proxy = get_proxy_url()
    uid = str(uuid.uuid4())
    # ä¸´æ—¶æ–‡ä»¶åŸºç¡€å
    temp_out_base = f"dl_sub_{uid}"
    
    # ç­–ç•¥ï¼šå§‹ç»ˆä¸‹è½½ vtt æ ¼å¼ï¼Œå› ä¸º content æ¥å£è¯æ˜äº† vtt ä¸‹è½½æ˜¯ç¨³å®šçš„
    ydl_opts = {
        'proxy': proxy,
        'skip_download': True,
        'writesubtitles': True,
        'writeautomaticsub': True,
        'subtitleslangs': [lang],
        'outtmpl': temp_out_base, # é»˜è®¤ä¼šè‡ªåŠ¨åŠ ä¸Š .lang.vtt
        'quiet': True,
        'postprocessors': [{ 'key': 'FFmpegSubtitlesConvertor', 'format': 'vtt' }]
    }

    actual_vtt_file = None
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            title = info.get('title', 'transcript')
            # æ¸…ç†æ–‡ä»¶åéæ³•å­—ç¬¦
            safe_title = re.sub(r'[\\/*?:"<>|]', "_", title).rstrip()

        # æŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶ (ç”±äº yt-dlp å‘½åè§„åˆ™å¤šå˜ï¼Œé‡‡ç”¨å‰ç¼€æ‰«ææ³•)
        for f in os.listdir('.'):
            if f.startswith(temp_out_base) and f.endswith('.vtt'):
                actual_vtt_file = f
                break

        if not actual_vtt_file or not os.path.exists(actual_vtt_file):
            return "Error: Subtitle file failed to generate.", 404

        # è¯»å–ä¸‹è½½å¥½çš„ VTT å†…å®¹
        with open(actual_vtt_file, 'r', encoding='utf-8') as f:
            vtt_content = f.read()
        
        # ä½¿ç”¨ä½ åŸæœ¬æˆåŠŸçš„è§£æé€»è¾‘
        segments = parse_vtt_to_segments(vtt_content)
        
        if not segments:
            return "Error: Subtitle content is empty.", 404

        if dl_type == 'txt':
            # è½¬æ¢ä¸º TXT æ ¼å¼å†…å®¹
            output_data = "\n".join([f"[{s['t']}] {s['txt']}" for s in segments])
            download_name = f"{safe_title}.txt"
            mimetype = 'text/plain'
        else:
            # è½¬æ¢ä¸º SRT æ ¼å¼å†…å®¹ (æ‰‹åŠ¨æ‹¼æ¥ï¼Œä¸ä¾èµ– FFmpeg)
            srt_lines = []
            for i, s in enumerate(segments):
                # å°† MM:SS è½¬æ¢ä¸º SRT æ ‡å‡†çš„ 00:MM:SS,000 æ ¼å¼
                # æ³¨æ„ï¼šè¿™é‡Œç®€å•å¡«å……äº†å°æ—¶å’Œæ¯«ç§’ä»¥ä¿è¯æ’­æ”¾å™¨å…¼å®¹æ€§
                timestamp = f"00:{s['t']},000"
                srt_lines.append(f"{i+1}")
                # ç»“æŸæ—¶é—´ç®€å•è®¾ä¸ºèµ·å§‹æ—¶é—´ï¼ˆå¤§éƒ¨åˆ†æ’­æ”¾å™¨ä¼šè‡ªåŠ¨å¤„ç†ï¼‰ï¼Œæˆ–æ ¹æ®ä¸‹ä¸€æ¡å¤„ç†
                srt_lines.append(f"{timestamp} --> {timestamp}")
                srt_lines.append(s['txt'])
                srt_lines.append("") # ç©ºè¡Œåˆ†éš”
            output_data = "\n".join(srt_lines)
            download_name = f"{safe_title}.srt"
            mimetype = 'application/x-subrip'

        # å°†å†…å®¹æ”¾å…¥å†…å­˜æµå‘é€ç»™ç”¨æˆ·
        mem_file = io.BytesIO(output_data.encode('utf-8'))
        
        return send_file(
            mem_file, 
            mimetype=mimetype, 
            as_attachment=True, 
            download_name=download_name
        )

    except Exception as e:
        print(f"Download error: {str(e)}")
        return f"Download failed: {str(e)}", 500
    finally:
        # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œå°è¯•æ¸…ç†ç£ç›˜ä¸Šçš„ä¸´æ—¶ vtt æ–‡ä»¶
        if actual_vtt_file and os.path.exists(actual_vtt_file):
            try:
                os.remove(actual_vtt_file)
            except:
                pass





















# ==============================================================================
# 7. å…¼å®¹æ€§ä¸ AI æ¥å£ (ä» app.py åˆå¹¶)
# ==============================================================================

@app.route('/', methods=['GET'])
def index():
    return "<h3>YouTube API Running (Full Support)</h3>"







@app.route('/api/download', methods=['GET', 'POST'])
def api_download_merged():
    """
    GET: æ—§ç‰ˆè§†é¢‘ä¸‹è½½
    POST: æ–°ç‰ˆå­—å¹•å•æ¡ä¸‹è½½ (main.js: downloadFile)
    """
    if request.method == 'GET':
        url = request.args.get('url')
        quality = request.args.get('quality', 'best')
        if not url: return jsonify({"code": 1, "message": "Missing url"}), 400
        
        filepath, filename, error = run_download_video_core(url, quality, DOWNLOAD_FOLDER)
        if error: return jsonify({"code": 1, "message": error}), 500
        
        @after_this_request
        def remove_file(response):
            try: os.remove(filepath)
            except: pass
            return response
        return send_file(filepath, as_attachment=True, download_name=filename)

    elif request.method == 'POST':
        data = request.get_json() or {}
        url = data.get('url')
        lang = data.get('lang', 'en')
        fmt = data.get('format', 'srt')
        title_hint = data.get('title', 'subtitle')
        if not url: return jsonify({"status": 1, "message": "Missing URL"}), 400
        content, filename, err = get_subtitle_content(url, lang, fmt)
        if err: return jsonify({"status": 1, "message": err}), 500
        if not filename: filename = f"{sanitize_filename(title_hint)}.{fmt}"
        mem_file = io.BytesIO()
        mem_file.write(content.encode('utf-8'))
        mem_file.seek(0)
        return send_file(mem_file, as_attachment=True, download_name=filename, mimetype='text/plain')

@app.route('/api/batch_check', methods=['POST'])
def api_batch_check():
    data = request.get_json() or {}
    urls = data.get('urls', [])
    if not urls: return jsonify({"status": "failed", "message": "No URLs provided"}), 400
    with ThreadPoolExecutor(max_workers=5) as ex:
        results = list(ex.map(check_video_for_batch, urls))
    return jsonify({"status": "completed", "results": results})

@app.route('/api/batch_submit', methods=['POST'])
def api_batch_submit():
    data = request.get_json() or {}
    videos = data.get('videos', [])
    lang = data.get('lang', 'en')
    fmt = data.get('format', 'srt')
    if not videos: return jsonify({"status": "failed", "message": "No videos"}), 400
    task_id = str(uuid.uuid4())
    tasks[task_id] = {"status": "pending", "progress": f"0/{len(videos)}", "total": len(videos), "type": "subtitle"}
    threading.Thread(target=batch_subtitle_worker, args=(task_id, videos, lang, fmt), daemon=True).start()
    return jsonify({"status": "pending", "task_id": task_id})

@app.route('/api/batch-download', methods=['POST'])
def api_batch_download_legacy():
    task_id = str(uuid.uuid4())
    videos = request.get_json().get('videos', [])
    tasks[task_id] = {"status": "pending", "progress": f"0/{len(videos)}", "type": "video"}
    threading.Thread(target=batch_video_worker, args=(task_id, videos), daemon=True).start()
    return jsonify({"code": 0, "task_id": task_id})

@app.route('/api/task_status', methods=['GET'])
def api_task_status_merged():
    task_id = request.args.get('task_id')
    task = tasks.get(task_id)
    if not task: return jsonify({"status": "not_found", "code": 1}), 404
    return jsonify({"code": 0, "status": task['status'], "progress": task['progress'], "data": task})

@app.route('/api/download_zip', methods=['GET', 'POST'])
def api_download_zip_merged():
    if request.method == 'POST':
        task_id = request.get_json().get('task_id')
    else:
        task_id = request.args.get('task_id')
    task = tasks.get(task_id)
    if not task or task.get('status') != 'completed':
        return jsonify({"status": 1, "message": "Task not ready or not found"}), 400
    zip_path = task.get('zip_path')
    zip_name = task.get('zip_name')
    if not zip_path or not os.path.exists(zip_path):
        return jsonify({"status": 1, "message": "File missing"}), 500
    @after_this_request
    def remove_file(response):
        try: os.remove(zip_path)
        except: pass
        return response
    return send_file(zip_path, as_attachment=True, download_name=zip_name)

@app.route('/api/subtitle', methods=['GET'])
def api_subtitle_legacy():
    url = request.args.get('url')
    lang = request.args.get('lang')
    content, filename, error = get_subtitle_content(url, lang, 'srt')
    if error: return jsonify({"code": 1, "message": error}), 500
    return send_file(io.BytesIO(content.encode('utf-8')), as_attachment=True, download_name=filename)

@app.route('/api/video_info', methods=['POST'])
def api_video_info():
    """å¿«é€Ÿè·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯ï¼Œä¸ä¸‹è½½å­—å¹•"""
    data = request.get_json() or {}
    video_url = data.get('url')
    if not video_url:
        return jsonify({"error": "No URL provided"}), 400
    
    proxy = get_proxy_url()
    ydl_opts = {
        'proxy': proxy,
        'quiet': True,
        'no_warnings': True,
        'skip_download': True,
        'extract_flat': False
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å­—å¹•
            has_subtitles = bool(info.get('subtitles') or info.get('automatic_captions'))
            
            return jsonify({
                "id": info.get('id'),
                "title": info.get('title', 'Unknown Title'),
                "uploader": info.get('uploader', 'Unknown'),
                "duration": info.get('duration', 0),
                "thumbnail": info.get('thumbnail'),
                "view_count": info.get('view_count'),
                "upload_date": info.get('upload_date'),
                "has_subtitles": has_subtitles,
                "description": info.get('description', '')[:500] + '...' if info.get('description') else ''
            })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/generate_summary_stream', methods=['POST'])
def api_generate_summary_stream():
    data = request.get_json() or {}
    video_url = data.get('url')
    if not video_url: 
        return jsonify({"status": "failed", "message": "Missing URL"}), 400
    
    print(f"ğŸš€ [AI Summary] Request received for: {video_url}")
    
    def generate():
        # 1. ç«‹å³å‘ŠçŸ¥å‰ç«¯æ­£åœ¨å·¥ä½œ
        yield "__STATUS__:Analyzing video structure...\n"
        start_time = time.time()

        try:
            # 2. åœ¨æµå†…éƒ¨è·å–å­—å¹• (è€—æ—¶æ“ä½œ)
            yield "__STATUS__:Extracting subtitles (this may take a few seconds)...\n"
            
            # è·å–å­—å¹•å†…å®¹
            transcript, title_info, err = get_subtitle_fast(video_url, 'en')
            
            if err:
                print(f"âš ï¸ [AI Summary] Subtitle fetch failed, trying description: {err}")
                yield "__STATUS__:Subtitles unavailable, falling back to description...\n"
                
                proxy = get_proxy_url()
                with yt_dlp.YoutubeDL({'proxy': proxy, 'skip_download': True, 'quiet': True}) as ydl:
                    info = ydl.extract_info(video_url, download=False)
                    video_title = info.get('title', 'Unknown Video')
                    video_duration = info.get('duration', 0)
                    transcript = info.get('description', '')
                    if len(transcript) < 200:
                        yield f"__ERROR__:No subtitles available and description is too short."
                        return
            else:
                video_title = title_info.rsplit('.', 1)[0] if title_info else 'Video'
                # ç®€å•ä¼°ç®—æˆ–é»˜è®¤å€¼ï¼Œé¿å…å†æ¬¡è¯·æ±‚
                video_duration = 0 

            print(f"ğŸ“Š [AI Summary] Transcript ready in {time.time() - start_time:.2f}s. Length: {len(transcript)}")
            yield "__STATUS__:Generating study guide...\n"

            # 3. å‡†å¤‡ AI è¯·æ±‚
            headers = {"Content-Type": "application/json", "Authorization": f"Bearer {ARK_API_KEY}"}
            
            system_prompt = """You are a content analyst for a video platform. Help users quickly understand what a video is about and whether it's worth their time.

STEP 1 â€” CLASSIFY the content type (use this internally to choose structure, do not output this step):
tutorial | lecture | interview | documentary | news | other

STEP 2 â€” CHOOSE structure based on type:
- tutorial    â†’ Overview, Prerequisites (if any), Key Steps, Common Mistakes, Result
- lecture     â†’ Core Argument, Supporting Points, Evidence & Examples, Conclusions
- interview   â†’ Guest & Context, Main Topics Discussed, Notable Quotes or Insights
- documentary/news â†’ Background, Key Facts, Implications
- other       â†’ 3-4 thematic sections with descriptive titles that reflect the actual content

STEP 3 â€” WRITE the summary:

# [Exact video title]

> [One sentence: what this video is and who should watch it]

## [Section titles based on content type â€” descriptive, not generic]
[2-4 sentences or 3-5 bullets per section. **Bold the single most important phrase** in each section.]

## Key Takeaways
- [3-5 specific points using actual names/numbers/examples from the video]

QUALITY RULES:
- Only use information present in the transcript â€” never infer or add outside context
- Specific beats generic: "reduced latency by 40ms" beats "improved performance"
- Match depth to video length: short video = concise; long video = thorough
- Forbidden phrases: "In this video", "The speaker discusses", "It's worth noting", "In conclusion"
- If transcript appears incomplete, add: *(Note: transcript may be incomplete)*"""

            user_prompt = f"""Summarize this video.

Title: {video_title}
Duration: approximately {max(1, video_duration // 60)} minutes

Transcript:
{transcript[:35000]}"""

            payload = {
                "model": ARK_MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": True,
                "temperature": 0.7,
                "max_tokens": 4000
            }
            
            response = requests.post(ARK_URL, headers=headers, json=payload, stream=True, timeout=180)
            thinking_notified = False
            content_started = False
            for line in response.iter_lines():
                if line:
                    chunk = line.decode('utf-8').strip()
                    if chunk.startswith("data: "):
                        data_str = chunk[6:]
                        if data_str == "[DONE]":
                            break
                        try:
                            resp_json = json.loads(data_str)
                            delta = resp_json['choices'][0]['delta']
                            # thinkingé˜¶æ®µï¼šæ¨¡å‹åœ¨æ€è€ƒï¼Œå‘é€çŠ¶æ€æç¤º
                            reasoning = delta.get('reasoning_content', '')
                            if reasoning and not thinking_notified:
                                yield "__STATUS__:AI is thinking deeply...\n"
                                thinking_notified = True
                            # contenté˜¶æ®µï¼šå®é™…è¾“å‡ºå†…å®¹
                            content = delta.get('content', '')
                            if content:
                                if not content_started:
                                    content_started = True
                                    print(f"ğŸ“ [AI Summary] Content streaming started")
                                yield content
                        except:
                            continue

        except Exception as e:
            print(f"âŒ [AI Summary] Error: {str(e)}")
            yield f"__ERROR__:Server Error: {str(e)}"

    # ä½¿ç”¨ stream_with_context ä¿æŒä¸Šä¸‹æ–‡
    res = Response(stream_with_context(generate()), mimetype='text/event-stream')
    res.headers['X-Accel-Buffering'] = 'no'
    res.headers['Cache-Control'] = 'no-cache'
    res.headers['Connection'] = 'keep-alive'
    return res

@app.route('/api/generate_study_cards_stream', methods=['POST'])
def api_generate_study_cards_stream():
    data = request.get_json() or {}
    video_url = data.get('url')
    if not video_url:
        return jsonify({"error": "Missing URL"}), 400

    def generate():
        yield "__STATUS__:Fetching subtitles...\n"
        try:
            transcript, title_info, err = get_subtitle_content(video_url, 'en', 'txt')
            if err:
                yield "__STATUS__:Subtitles unavailable, using description...\n"
                proxy = get_proxy_url()
                with yt_dlp.YoutubeDL({'proxy': proxy, 'skip_download': True, 'quiet': True}) as ydl:
                    info = ydl.extract_info(video_url, download=False)
                    video_title = info.get('title', 'Unknown Video')
                    transcript = info.get('description', '')
                    if len(transcript) < 200:
                        yield "__ERROR__:No subtitles available and description is too short."
                        return
            else:
                video_title = title_info.rsplit('.', 1)[0] if title_info else 'Video'

            yield "__STATUS__:Generating study cards...\n"

            headers = {"Content-Type": "application/json", "Authorization": f"Bearer {ARK_API_KEY}"}
            system_prompt = """You are a study card generator. Create high-quality flashcards from video transcripts.

Output ONLY cards in this exact format, one after another with no other text:
---
Q: [question that tests understanding]
A: [comprehensive answer, 50-150 words]
T: [timestamp MM:SS or null]
Type: concept|definition|insight|action
Category: technical|business|design|general
---

Rules:
- Generate 6-8 cards
- Questions must require understanding, not just recall
- Answers must be educational and specific
- No intro text, no summary, ONLY the card blocks"""

            user_prompt = f"""Create 6-8 study cards from this transcript.

Video: {video_title}
Transcript: {transcript[:35000]}

Output ONLY the card blocks in the specified format."""

            payload = {
                "model": ARK_MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": True,
                "temperature": 0.7,
                "max_tokens": 4000
            }

            response = requests.post(ARK_URL, headers=headers, json=payload, stream=True, timeout=180)
            thinking_notified = False
            content_started = False
            for line in response.iter_lines():
                if line:
                    chunk = line.decode('utf-8').strip()
                    if chunk.startswith("data: "):
                        data_str = chunk[6:]
                        if data_str == "[DONE]":
                            break
                        try:
                            resp_json = json.loads(data_str)
                            delta = resp_json['choices'][0]['delta']
                            # thinkingé˜¶æ®µï¼šæ¨¡å‹åœ¨æ€è€ƒï¼Œå‘é€çŠ¶æ€æç¤º
                            reasoning = delta.get('reasoning_content', '')
                            if reasoning and not thinking_notified:
                                yield "__STATUS__:AI is thinking deeply...\n"
                                thinking_notified = True
                            # contenté˜¶æ®µï¼šå®é™…è¾“å‡ºå†…å®¹
                            content = delta.get('content', '')
                            if content:
                                if not content_started:
                                    content_started = True
                                    print(f"ğŸ“ [Study Cards] Content streaming started")
                                yield content
                        except:
                            continue

        except Exception as e:
            print(f"âŒ [Study Cards] Error: {str(e)}")
            yield f"__ERROR__:Server Error: {str(e)}"

    res = Response(stream_with_context(generate()), mimetype='text/plain')
    res.headers['X-Accel-Buffering'] = 'no'
    res.headers['Cache-Control'] = 'no-cache'
    res.headers['Connection'] = 'keep-alive'
    return res

@app.route('/parse', methods=['POST', 'OPTIONS'])
def parse_video_legacy():
    if request.method == 'OPTIONS': return '', 204
    video_url = request.json.get('url')
    if not video_url: return jsonify({"error": "No URL provided"}), 400
    ydl_opts = {
        'format': '22/18/best', 'quiet': False,
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'nocheckcertificate': True, 'source_address': '0.0.0.0',
    }
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=False)
            return jsonify({"title": info.get('title'), "url": info.get('url'), "thumbnail": info.get('thumbnail'), "ext": info.get('ext') or 'mp4'})
    except Exception as e: return jsonify({"error": str(e)}), 500

# connectç½‘ç«™å»é™¤èƒŒæ™¯api
@app.route('/api/remove-background', methods=['POST'])
def remove_background_api():
    file = request.files.get('file')
    if not file: return jsonify({"error": "No file"}), 400
    try:
        output_data = remove(file.read())
        return send_file(io.BytesIO(output_data), mimetype='image/png', as_attachment=True, download_name='no_bg.png')
    except Exception as e: return jsonify({"error": str(e)}), 500




# ==============================================================================
# 8. å¯åŠ¨é…ç½®
# ==============================================================================
if __name__ == '__main__':
    # éƒ¨ç½²åˆ°æœåŠ¡å™¨å¿…é¡»ç›‘å¬ 0.0.0.0ï¼Œç«¯å£å»ºè®® 5000
    print(f"ğŸ“¡ YouTube è§£ææœåŠ¡æ­£åœ¨è¿è¡Œåœ¨ 0.0.0.0:5000...")
    app.run(host='0.0.0.0', port=5000, debug=False)